{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 38: ML Security Introduction\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/depalmar/ai_for_the_win/blob/main/notebooks/lab38_ml_security_intro.ipynb)\n\nIntroduction to machine learning security concepts and attack vectors.\n\n## Learning Objectives\n- Understand the ML security threat landscape\n- Learn common attack types (evasion, poisoning, extraction)\n- Identify vulnerabilities in ML pipelines\n- Build secure ML development practices\n\n**Next:** Lab 39 (Adversarial ML)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies (Colab only)\n",
    "#@markdown Run this cell to install required packages in Colab\n",
    "\n",
    "%pip install -q numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"‚úÖ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Attack ML Systems?\n",
    "\n",
    "### The Stakes Are High\n",
    "\n",
    "| Domain | ML Application | Attack Impact |\n",
    "|--------|---------------|---------------|\n",
    "| **Security** | Malware detection | Malware evades detection |\n",
    "| **Finance** | Fraud detection | Fraudulent transactions pass |\n",
    "| **Healthcare** | Diagnosis | Wrong treatment decisions |\n",
    "| **Content** | Spam/abuse filters | Abuse content gets through |\n",
    "\n",
    "### The ML Attack Surface\n",
    "\n",
    "```\n",
    "DATA COLLECTION ‚Üí PREPROCESSING ‚Üí TRAINING ‚Üí DEPLOYMENT ‚Üí INFERENCE\n",
    "       ‚îÇ               ‚îÇ             ‚îÇ            ‚îÇ            ‚îÇ\n",
    "       ‚ñº               ‚ñº             ‚ñº            ‚ñº            ‚ñº\n",
    "   Poisoning      Poisoning      Backdoor     Extraction   Evasion\n",
    "   via source     via pipeline   via trojan   via API      attacks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Type 1: Evasion\n",
    "\n",
    "**Goal**: Craft an input that's misclassified at inference time.\n",
    "\n",
    "```\n",
    "Original malware ‚Üí ADD PERTURBATION ‚Üí Modified malware\n",
    "     ‚îÇ                                      ‚îÇ\n",
    "     ‚ñº                                      ‚ñº\n",
    "\"MALICIOUS\" (correct)              \"BENIGN\" (wrong!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating Evasion Attack\n",
    "\n",
    "# Simple classifier: detect \"malware\" based on features\n",
    "# Feature 1: Number of suspicious API calls\n",
    "# Feature 2: Entropy level\n",
    "\n",
    "# Training data: [suspicious_apis, entropy] -> label\n",
    "X_train = np.array([\n",
    "    [10, 7.5],  # Malware\n",
    "    [8, 7.2],   # Malware\n",
    "    [12, 7.8],  # Malware\n",
    "    [2, 4.5],   # Benign\n",
    "    [1, 5.0],   # Benign\n",
    "    [3, 4.8],   # Benign\n",
    "])\n",
    "y_train = np.array([1, 1, 1, 0, 0, 0])\n",
    "\n",
    "# Train classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Original malware sample\n",
    "original_malware = np.array([[9, 7.3]])\n",
    "original_pred = classifier.predict(original_malware)[0]\n",
    "original_prob = classifier.predict_proba(original_malware)[0][1]\n",
    "\n",
    "print(\"üéØ EVASION ATTACK DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìç Original Malware Sample:\")\n",
    "print(f\"   Features: suspicious_apis={original_malware[0][0]}, entropy={original_malware[0][1]}\")\n",
    "print(f\"   Prediction: {'MALICIOUS' if original_pred == 1 else 'BENIGN'}\")\n",
    "print(f\"   Confidence: {original_prob:.1%}\")\n",
    "\n",
    "# Attacker's evasion: add \"benign-looking\" features\n",
    "# Strategy: Reduce apparent suspicious APIs, lower entropy appearance\n",
    "evaded_malware = np.array([[4, 5.5]])  # Changed features while keeping malicious behavior\n",
    "\n",
    "evaded_pred = classifier.predict(evaded_malware)[0]\n",
    "evaded_prob = classifier.predict_proba(evaded_malware)[0][1]\n",
    "\n",
    "print(f\"\\nüìç EVASION ATTEMPT:\")\n",
    "print(f\"   Modified Features: suspicious_apis={evaded_malware[0][0]}, entropy={evaded_malware[0][1]}\")\n",
    "print(f\"   Prediction: {'MALICIOUS' if evaded_pred == 1 else 'BENIGN'}\")\n",
    "print(f\"   Confidence: {evaded_prob:.1%}\")\n",
    "\n",
    "if evaded_pred == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è EVASION SUCCESSFUL! Malware classified as benign.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Type 2: Poisoning\n",
    "\n",
    "**Goal**: Corrupt training data to degrade model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating Poisoning Attack\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Clean training data\n",
    "X_clean = np.array([\n",
    "    [10, 7.5], [8, 7.2], [12, 7.8], [9, 7.0], [11, 7.6],  # Malware\n",
    "    [2, 4.5], [1, 5.0], [3, 4.8], [2, 4.2], [1, 4.5],      # Benign\n",
    "])\n",
    "y_clean = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Test data\n",
    "X_test = np.array([[9, 7.1], [2, 4.6], [10, 7.4], [1, 4.8]])\n",
    "y_test = np.array([1, 0, 1, 0])\n",
    "\n",
    "# Train on clean data\n",
    "clean_model = LogisticRegression()\n",
    "clean_model.fit(X_clean, y_clean)\n",
    "clean_accuracy = accuracy_score(y_test, clean_model.predict(X_test))\n",
    "\n",
    "print(\"üß™ POISONING ATTACK DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìç Clean Model Performance:\")\n",
    "print(f\"   Test Accuracy: {clean_accuracy:.1%}\")\n",
    "\n",
    "# Poisoned data: attacker injects mislabeled samples\n",
    "# Adding malware samples labeled as \"benign\"\n",
    "X_poisoned = np.vstack([X_clean, [[9, 7.3], [10, 7.5]]])  # Adding malware features\n",
    "y_poisoned = np.concatenate([y_clean, [0, 0]])  # But labeling them as benign!\n",
    "\n",
    "# Train on poisoned data\n",
    "poisoned_model = LogisticRegression()\n",
    "poisoned_model.fit(X_poisoned, y_poisoned)\n",
    "poisoned_accuracy = accuracy_score(y_test, poisoned_model.predict(X_test))\n",
    "\n",
    "print(f\"\\nüìç Poisoned Model Performance:\")\n",
    "print(f\"   Test Accuracy: {poisoned_accuracy:.1%}\")\n",
    "print(f\"   Accuracy Drop: {(clean_accuracy - poisoned_accuracy):.1%}\")\n",
    "\n",
    "if poisoned_accuracy < clean_accuracy:\n",
    "    print(f\"\\n‚ö†Ô∏è POISONING SUCCESSFUL! Model accuracy degraded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defense Strategies\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Defense | Against | How |\n",
    "|---------|---------|-----|\n",
    "| **Adversarial Training** | Evasion | Train on perturbed examples |\n",
    "| **Input Validation** | Evasion | Detect anomalous inputs |\n",
    "| **Data Sanitization** | Poisoning | Filter training data |\n",
    "| **Ensemble Models** | All | Harder to attack multiple models |\n",
    "| **Rate Limiting** | Extraction | Detect systematic queries |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Defense: Input Validation\n",
    "\n",
    "def validate_input(x: np.ndarray, model, threshold: float = 0.3) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Detect potentially adversarial inputs based on prediction confidence.\n",
    "\n",
    "    Low confidence predictions may indicate:\n",
    "    - Adversarial examples designed to confuse the model\n",
    "    - Out-of-distribution inputs\n",
    "    - Inputs near the decision boundary\n",
    "\n",
    "    Args:\n",
    "        x: Input features\n",
    "        model: Trained classifier\n",
    "        threshold: Minimum confidence required\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (is_valid, reason)\n",
    "    \"\"\"\n",
    "    proba = model.predict_proba(x)[0]\n",
    "    confidence = max(proba)\n",
    "\n",
    "    if confidence < (0.5 + threshold):\n",
    "        return False, f\"Low confidence ({confidence:.1%}) - possible adversarial input\"\n",
    "\n",
    "    return True, f\"Input appears valid (confidence: {confidence:.1%})\"\n",
    "\n",
    "# Test validation\n",
    "print(\"üõ°Ô∏è DEFENSE: INPUT VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_inputs = [\n",
    "    np.array([[10, 7.5]]),  # Clear malware\n",
    "    np.array([[2, 4.5]]),   # Clear benign\n",
    "    np.array([[5, 6.0]]),   # Ambiguous (near decision boundary)\n",
    "]\n",
    "\n",
    "for i, test in enumerate(test_inputs):\n",
    "    is_valid, reason = validate_input(test, clean_model)\n",
    "    status = \"‚úÖ VALID\" if is_valid else \"‚ö†Ô∏è FLAGGED\"\n",
    "    print(f\"\\nInput {i+1}: {status}\")\n",
    "    print(f\"   {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Key Takeaways\n",
    "\n",
    "1. **ML systems are targets** - Security, finance, anywhere ML makes decisions\n",
    "2. **Know your attack surface** - Data, training, deployment, inference\n",
    "3. **Evasion is most common** - Attackers craft inputs to bypass ML\n",
    "4. **Defense in depth** - No single defense is sufficient\n",
    "5. **Monitor and adapt** - Attackers evolve, so must defenses\n",
    "\n",
    "## Attacker Knowledge Levels\n",
    "\n",
    "| Level | What Attacker Knows | Attack Difficulty |\n",
    "|-------|---------------------|-------------------|\n",
    "| **White-box** | Full model access | Easier |\n",
    "| **Gray-box** | Partial knowledge | Medium |\n",
    "| **Black-box** | Only query access | Harder |\n",
    "\n",
    "Most real attacks are **black-box** - attacker only has API access.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Lab 17**: Implement more sophisticated attacks (FGSM, PGD)\n",
    "- **Lab 18**: Build robust ML models\n",
    "- **Lab 49**: Apply these concepts to LLM security"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
