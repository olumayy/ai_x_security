{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 24: Monitoring AI Systems\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/depalmar/ai_for_the_win/blob/main/notebooks/lab24_monitoring_ai.ipynb)\n\nLearn to monitor, log, and track costs for AI-powered security systems.\n\n## Learning Objectives\n- Implement logging for LLM calls\n- Track token usage and costs\n- Monitor response times and errors\n- Build dashboards for AI system health\n\n## Why Monitor AI Systems?\n\nProduction AI systems need observability for:\n- **Cost control**: LLM APIs charge per token\n- **Performance**: Latency affects user experience\n- **Quality**: Detect degradation or hallucinations\n- **Security**: Detect prompt injection attempts\n\n**Next:** Lab 29 (IR Copilot) or Lab 25 (DFIR Fundamentals primer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies (Colab only)\n",
    "%pip install -q anthropic openai google-generativeai python-dotenv pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    for key in [\"ANTHROPIC_API_KEY\", \"OPENAI_API_KEY\", \"GOOGLE_API_KEY\"]:\n",
    "        try:\n",
    "            os.environ[key] = userdata.get(key)\n",
    "        except:\n",
    "            pass\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create a Logging Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LLMCall:\n",
    "    \"\"\"Record of a single LLM API call.\"\"\"\n",
    "    timestamp: str\n",
    "    provider: str\n",
    "    model: str\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_ms: float\n",
    "    cost_usd: float\n",
    "    success: bool\n",
    "    error: str = None\n",
    "\n",
    "# Token pricing (as of 2025)\n",
    "PRICING = {\n",
    "    \"claude-sonnet-4.5\": {\"input\": 3.0, \"output\": 15.0},  # per 1M tokens\n",
    "    \"claude-haiku-4.5\": {\"input\": 0.80, \"output\": 4.0},\n",
    "    \"claude-opus-4.5\": {\"input\": 15.0, \"output\": 75.0},\n",
    "    \"gpt-5\": {\"input\": 5.0, \"output\": 15.0},\n",
    "    \"gpt-5-mini\": {\"input\": 0.30, \"output\": 1.20},\n",
    "    \"gemini-3-flash\": {\"input\": 0.10, \"output\": 0.40},\n",
    "}\n",
    "\n",
    "def calculate_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:\n",
    "    \"\"\"Calculate cost in USD.\"\"\"\n",
    "    if model not in PRICING:\n",
    "        return 0.0\n",
    "    pricing = PRICING[model]\n",
    "    input_cost = (prompt_tokens / 1_000_000) * pricing[\"input\"]\n",
    "    output_cost = (completion_tokens / 1_000_000) * pricing[\"output\"]\n",
    "    return input_cost + output_cost\n",
    "\n",
    "# Global log storage\n",
    "CALL_LOG: List[LLMCall] = []\n",
    "\n",
    "print(\"‚úÖ Logging infrastructure ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitored_llm_call(prompt: str, system: str = \"You are a helpful assistant.\") -> str:\n",
    "    \"\"\"Make an LLM call with full monitoring.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Detect provider\n",
    "    if os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "        provider, model = \"anthropic\", \"claude-sonnet-4.5\"\n",
    "    elif os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        provider, model = \"openai\", \"gpt-5\"\n",
    "    elif os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "        provider, model = \"google\", \"gemini-3-flash\"\n",
    "    else:\n",
    "        raise ValueError(\"No API key found. Add ANTHROPIC_API_KEY, OPENAI_API_KEY, or GOOGLE_API_KEY to Colab Secrets.\")\n",
    "    \n",
    "    try:\n",
    "        if provider == \"anthropic\":\n",
    "            from anthropic import Anthropic\n",
    "            client = Anthropic()\n",
    "            response = client.messages.create(\n",
    "                model=model, max_tokens=1024, system=system,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            prompt_tokens = response.usage.input_tokens\n",
    "            completion_tokens = response.usage.output_tokens\n",
    "            result = response.content[0].text\n",
    "            \n",
    "        elif provider == \"openai\":\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=model, max_tokens=1024,\n",
    "                messages=[{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            prompt_tokens = response.usage.prompt_tokens\n",
    "            completion_tokens = response.usage.completion_tokens\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "        elif provider == \"google\":\n",
    "            import google.generativeai as genai\n",
    "            genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "            model_instance = genai.GenerativeModel(model)\n",
    "            response = model_instance.generate_content(f\"{system}\\n\\n{prompt}\")\n",
    "            prompt_tokens = len(f\"{system}\\n\\n{prompt}\") // 4\n",
    "            completion_tokens = len(response.text) // 4\n",
    "            result = response.text\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        cost = calculate_cost(model, prompt_tokens, completion_tokens)\n",
    "        \n",
    "        # Log the call\n",
    "        call_record = LLMCall(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            provider=provider,\n",
    "            model=model,\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            completion_tokens=completion_tokens,\n",
    "            total_tokens=prompt_tokens + completion_tokens,\n",
    "            latency_ms=latency_ms,\n",
    "            cost_usd=cost,\n",
    "            success=True\n",
    "        )\n",
    "        CALL_LOG.append(call_record)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        call_record = LLMCall(\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            provider=provider,\n",
    "            model=model,\n",
    "            prompt_tokens=0,\n",
    "            completion_tokens=0,\n",
    "            total_tokens=0,\n",
    "            latency_ms=latency_ms,\n",
    "            cost_usd=0,\n",
    "            success=False,\n",
    "            error=str(e)\n",
    "        )\n",
    "        CALL_LOG.append(call_record)\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ Monitored LLM function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make Some Monitored Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some security analysis calls\n",
    "test_prompts = [\n",
    "    \"Analyze this log entry: Failed login from 192.168.1.100\",\n",
    "    \"What are the indicators of a phishing email?\",\n",
    "    \"Generate a YARA rule for detecting Emotet\",\n",
    "    \"Explain the MITRE ATT&CK technique T1059.001\",\n",
    "    \"Is the IP 45.33.32.156 associated with any known threats?\",\n",
    "]\n",
    "\n",
    "print(\"Making monitored LLM calls...\\n\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"Call {i}: {prompt[:50]}...\")\n",
    "    try:\n",
    "        result = monitored_llm_call(prompt)\n",
    "        print(f\"  ‚úÖ Success ({CALL_LOG[-1].latency_ms:.0f}ms, ${CALL_LOG[-1].cost_usd:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error: {e}\")\n",
    "\n",
    "print(f\"\\nTotal calls logged: {len(CALL_LOG)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze the Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame([asdict(call) for call in CALL_LOG])\n",
    "\n",
    "print(\"üìä Monitoring Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total calls: {len(df)}\")\n",
    "print(f\"Successful: {df['success'].sum()}\")\n",
    "print(f\"Failed: {(~df['success']).sum()}\")\n",
    "print()\n",
    "print(f\"Total tokens: {df['total_tokens'].sum():,}\")\n",
    "print(f\"Total cost: ${df['cost_usd'].sum():.4f}\")\n",
    "print()\n",
    "print(f\"Avg latency: {df['latency_ms'].mean():.0f}ms\")\n",
    "print(f\"Max latency: {df['latency_ms'].max():.0f}ms\")\n",
    "print(f\"Min latency: {df['latency_ms'].min():.0f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Latency distribution\n",
    "axes[0].bar(range(len(df)), df['latency_ms'])\n",
    "axes[0].set_xlabel('Call #')\n",
    "axes[0].set_ylabel('Latency (ms)')\n",
    "axes[0].set_title('Latency per Call')\n",
    "axes[0].axhline(y=df['latency_ms'].mean(), color='r', linestyle='--', label='Mean')\n",
    "axes[0].legend()\n",
    "\n",
    "# Token usage\n",
    "axes[1].bar(range(len(df)), df['total_tokens'])\n",
    "axes[1].set_xlabel('Call #')\n",
    "axes[1].set_ylabel('Tokens')\n",
    "axes[1].set_title('Token Usage per Call')\n",
    "\n",
    "# Cumulative cost\n",
    "axes[2].plot(df['cost_usd'].cumsum(), marker='o')\n",
    "axes[2].set_xlabel('Call #')\n",
    "axes[2].set_ylabel('Cumulative Cost ($)')\n",
    "axes[2].set_title('Cumulative Cost')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alerts(call_log: List[LLMCall], thresholds: Dict) -> List[str]:\n",
    "    \"\"\"Check for alert conditions.\"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    df = pd.DataFrame([asdict(call) for call in call_log])\n",
    "    \n",
    "    # Cost alert\n",
    "    total_cost = df['cost_usd'].sum()\n",
    "    if total_cost > thresholds.get('max_cost', 1.0):\n",
    "        alerts.append(f\"üö® COST ALERT: ${total_cost:.2f} exceeds threshold\")\n",
    "    \n",
    "    # Latency alert\n",
    "    avg_latency = df['latency_ms'].mean()\n",
    "    if avg_latency > thresholds.get('max_latency_ms', 5000):\n",
    "        alerts.append(f\"üö® LATENCY ALERT: {avg_latency:.0f}ms exceeds threshold\")\n",
    "    \n",
    "    # Error rate alert\n",
    "    error_rate = (~df['success']).mean()\n",
    "    if error_rate > thresholds.get('max_error_rate', 0.1):\n",
    "        alerts.append(f\"üö® ERROR RATE ALERT: {error_rate:.1%} exceeds threshold\")\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# Check alerts\n",
    "thresholds = {\n",
    "    'max_cost': 0.10,  # $0.10\n",
    "    'max_latency_ms': 3000,  # 3 seconds\n",
    "    'max_error_rate': 0.05  # 5%\n",
    "}\n",
    "\n",
    "alerts = check_alerts(CALL_LOG, thresholds)\n",
    "if alerts:\n",
    "    print(\"‚ö†Ô∏è Active Alerts:\")\n",
    "    for alert in alerts:\n",
    "        print(f\"  {alert}\")\n",
    "else:\n",
    "    print(\"‚úÖ All systems nominal - no alerts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Add prompt logging\n",
    "Extend the monitoring to log the actual prompts (useful for debugging).\n",
    "\n",
    "### Exercise 2: Detect anomalies\n",
    "Add anomaly detection for sudden spikes in token usage or latency.\n",
    "\n",
    "### Exercise 3: Export to monitoring system\n",
    "Export metrics to Prometheus/Grafana format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Lab 23**: Build a full detection pipeline with monitoring\n",
    "- **Lab 10**: Add monitoring to your IR Copilot\n",
    "- **Lab 49**: Monitor for prompt injection attempts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
