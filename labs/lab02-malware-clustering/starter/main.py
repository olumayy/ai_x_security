#!/usr/bin/env python3
"""
Lab 02: Malware Sample Clustering - Starter Code

Use unsupervised learning to cluster malware samples by behavior and characteristics.

Instructions:
1. Complete each TODO section
2. Run with sample data in data/ folder
3. Compare your results with the solution
"""

from pathlib import Path
from typing import List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import LabelEncoder, StandardScaler

# =============================================================================
# Task 1: Load Sample Data
# =============================================================================


def load_malware_data(filepath: str) -> pd.DataFrame:
    """
    Load pre-extracted malware features.

    Features include:
    - file_size: Size in bytes
    - entropy: Shannon entropy
    - num_imports: Number of imported functions
    - num_sections: Number of PE sections
    - has_debug: Debug info present
    - has_signature: Digital signature present
    - imphash: Import hash (categorical)
    - family: Known malware family (for evaluation)

    TODO: Ask your AI assistant:
    "Write Python code to load a CSV file from the given filepath into a pandas
    DataFrame, handle any missing values by filling numeric columns with their
    median and categorical columns with 'unknown', print basic dataset statistics
    (shape, dtypes, missing value counts), and return the DataFrame."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


def explore_data(df: pd.DataFrame) -> None:
    """
    Print exploratory statistics about the dataset.

    TODO: Ask your AI assistant:
    "Write Python code to explore a pandas DataFrame by printing its shape and
    column names, showing value counts for categorical columns (like 'family'
    and 'imphash'), displaying descriptive statistics for numeric columns, and
    checking for any remaining missing values."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


# =============================================================================
# Task 2: Feature Engineering
# =============================================================================


def engineer_features(df: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
    """
    Prepare features for clustering.

    TODO: Ask your AI assistant:
    "Write Python code to prepare features for clustering from a malware DataFrame.
    Select numeric features (file_size, entropy, num_imports, num_sections, has_debug,
    has_signature), apply log transform to file_size since it's highly skewed, use
    LabelEncoder to convert the 'imphash' categorical column to numeric, combine all
    features into a matrix, apply StandardScaler to normalize the features, and return
    the scaled feature matrix as a numpy array along with the list of feature names."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


# =============================================================================
# Task 3: Dimensionality Reduction
# =============================================================================


def reduce_dimensions(X: np.ndarray, method: str = "pca") -> np.ndarray:
    """
    Reduce feature dimensions for visualization.

    Args:
        X: Feature matrix
        method: 'pca', 'tsne', or 'umap'

    Returns:
        2D representation

    TODO: Ask your AI assistant:
    "Write Python code to reduce a feature matrix to 2 dimensions for visualization.
    Support three methods: 'pca' using PCA(n_components=2), 'tsne' using TSNE with
    perplexity=30, and 'umap' using UMAP if available. If the input has more than
    50 dimensions, first reduce to 50 using PCA before applying t-SNE or UMAP.
    Return the 2D coordinates as a numpy array."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


# =============================================================================
# Task 4: Clustering
# =============================================================================


def find_optimal_k(X: np.ndarray, k_range: range = range(2, 10)) -> int:
    """
    Find optimal number of clusters using silhouette score.

    TODO: Ask your AI assistant:
    "Write Python code to find the optimal number of clusters for KMeans. Iterate
    through the given k_range, fit KMeans for each k value, calculate the silhouette
    score, track the best score and corresponding k, and return the optimal k value."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


def cluster_samples(X: np.ndarray, method: str = "kmeans", n_clusters: int = None) -> np.ndarray:
    """
    Cluster malware samples.

    Args:
        X: Feature matrix (or reduced features)
        method: 'kmeans', 'dbscan', or 'hierarchical'
        n_clusters: Number of clusters (for kmeans)

    Returns:
        Cluster labels

    TODO: Ask your AI assistant:
    "Write Python code to cluster samples using the specified method. For 'kmeans',
    if n_clusters is not provided, call find_optimal_k to determine it, then fit
    KMeans. For 'dbscan', use DBSCAN with eps=0.5 and min_samples=5. Print the
    silhouette score to evaluate clustering quality (if more than 1 cluster exists).
    Return the cluster labels as a numpy array."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


# =============================================================================
# Task 5: Visualization
# =============================================================================


def visualize_clusters(
    X_2d: np.ndarray,
    labels: np.ndarray,
    title: str = "Malware Clusters",
    save_path: str = None,
) -> None:
    """
    Visualize clustering results.

    TODO: Ask your AI assistant:
    "Write Python code to create a scatter plot visualization of clustering results.
    Use the 2D coordinates (X_2d) for x and y positions, color points by their cluster
    labels, add a colorbar or legend, set the title, label axes as 'Dimension 1' and
    'Dimension 2', and optionally save the figure to save_path if provided. Use
    matplotlib and a visually distinct colormap like 'tab10' or 'Set1'."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


def compare_with_families(
    X_2d: np.ndarray, cluster_labels: np.ndarray, family_labels: np.ndarray
) -> None:
    """
    Compare clustering with known malware families.

    TODO: Ask your AI assistant:
    "Write Python code to create a side-by-side comparison of cluster assignments
    versus actual malware family labels. Create a figure with 2 subplots: the left
    showing points colored by cluster_labels with title 'Predicted Clusters', the
    right showing points colored by family_labels with title 'Actual Families'.
    Calculate and print the Adjusted Rand Index (ARI) and Normalized Mutual Information
    (NMI) to measure agreement between clustering and true labels."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


# =============================================================================
# Task 6: Analysis
# =============================================================================


def analyze_clusters(df: pd.DataFrame, labels: np.ndarray) -> dict:
    """
    Analyze characteristics of each cluster.

    Returns:
        Dict with cluster statistics:
        - size: Number of samples
        - avg_entropy: Average entropy
        - avg_size: Average file size
        - common_features: Distinguishing features
        - suspected_family: Likely malware family

    TODO: Ask your AI assistant:
    "Write Python code to analyze each cluster's characteristics. Add the cluster
    labels to the DataFrame, group by cluster, and for each cluster calculate:
    the number of samples (size), average entropy, average file size, most common
    'imphash' value, and if 'family' column exists, the most common family (suspected
    family). Return a dictionary where keys are cluster numbers and values are dicts
    containing these statistics."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


def print_cluster_report(analysis: dict) -> None:
    """Print formatted cluster analysis report.

    TODO: Ask your AI assistant:
    "Write Python code to print a formatted report of cluster analysis results.
    Iterate through the analysis dictionary, and for each cluster print its ID,
    size, average entropy, average file size, and suspected malware family in a
    readable format with clear headers and separators."

    Then review and test the generated code.
    """
    # YOUR CODE HERE
    pass


# =============================================================================
# Main Execution
# =============================================================================


def main():
    """Main execution flow."""
    print("=" * 60)
    print("Lab 02: Malware Sample Clustering")
    print("=" * 60)

    # Load data
    data_path = Path(__file__).parent.parent / "data" / "malware_features.csv"

    if not data_path.exists():
        print(f"Data file not found: {data_path}")
        print("Creating sample data...")
        create_sample_data(data_path)

    print("\n[Step 1] Loading data...")
    df = load_malware_data(str(data_path))

    if df is None:
        print("Error: load_malware_data() returned None. Complete the TODO!")
        return

    explore_data(df)

    # Feature engineering
    print("\n[Step 2] Engineering features...")
    result = engineer_features(df)

    if result is None:
        print("Error: engineer_features() returned None. Complete the TODO!")
        return

    X, feature_names = result
    print(f"Feature matrix shape: {X.shape}")

    # Dimensionality reduction
    print("\n[Step 3] Reducing dimensions...")
    X_2d = reduce_dimensions(X, method="tsne")

    if X_2d is None:
        print("Error: reduce_dimensions() returned None. Complete the TODO!")
        return

    # Clustering
    print("\n[Step 4] Clustering samples...")
    labels = cluster_samples(X, method="kmeans")

    if labels is None:
        print("Error: cluster_samples() returned None. Complete the TODO!")
        return

    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    print(f"Found {n_clusters} clusters")

    # Calculate silhouette score
    if n_clusters > 1:
        score = silhouette_score(X, labels)
        print(f"Silhouette Score: {score:.3f}")

    # Visualization
    print("\n[Step 5] Creating visualizations...")
    visualize_clusters(X_2d, labels, title="Malware Clusters")

    # Compare with known families if available
    if "family" in df.columns:
        family_encoder = LabelEncoder()
        family_labels = family_encoder.fit_transform(df["family"])
        compare_with_families(X_2d, labels, family_labels)

    # Analysis
    print("\n[Step 6] Analyzing clusters...")
    analysis = analyze_clusters(df, labels)

    if analysis:
        print_cluster_report(analysis)

    print("\n" + "=" * 60)
    print("Clustering complete!")
    print("=" * 60)


def create_sample_data(filepath: Path):
    """Create sample malware features dataset."""
    np.random.seed(42)

    # Simulate different malware families
    families = ["emotet", "trickbot", "ryuk", "cobalt_strike", "generic"]
    n_samples = 500

    data = []
    for i in range(n_samples):
        family = np.random.choice(families, p=[0.25, 0.20, 0.15, 0.15, 0.25])

        # Family-specific characteristics
        if family == "emotet":
            file_size = np.random.lognormal(12, 0.5)
            entropy = np.random.normal(7.2, 0.3)
            num_imports = np.random.randint(30, 60)
        elif family == "trickbot":
            file_size = np.random.lognormal(11, 0.4)
            entropy = np.random.normal(6.8, 0.4)
            num_imports = np.random.randint(40, 80)
        elif family == "ryuk":
            file_size = np.random.lognormal(13, 0.6)
            entropy = np.random.normal(7.5, 0.2)
            num_imports = np.random.randint(20, 40)
        elif family == "cobalt_strike":
            file_size = np.random.lognormal(10, 0.3)
            entropy = np.random.normal(7.8, 0.2)
            num_imports = np.random.randint(10, 25)
        else:
            file_size = np.random.lognormal(11.5, 0.8)
            entropy = np.random.normal(6.5, 0.6)
            num_imports = np.random.randint(20, 100)

        data.append(
            {
                "sha256": f"sample_{i:04d}",
                "file_size": int(file_size),
                "entropy": min(8.0, max(0.0, entropy)),
                "num_imports": num_imports,
                "num_sections": np.random.randint(3, 8),
                "has_debug": np.random.choice([0, 1], p=[0.9, 0.1]),
                "has_signature": np.random.choice([0, 1], p=[0.8, 0.2]),
                "imphash": f"hash_{np.random.randint(0, 50):03d}",
                "family": family,
            }
        )

    df = pd.DataFrame(data)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(filepath, index=False)
    print(f"Created sample data with {len(df)} samples")


if __name__ == "__main__":
    main()
